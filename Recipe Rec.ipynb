{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from numpy import median\n",
    "\n",
    "dataDir = \"/Users/shiqi/Downloads/cse258/assignment1/\"\n",
    "path = dataDir + \"trainInteractions.csv.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "header\n",
    "# ['user_id,recipe_id,date,rating']\n",
    "dataset = []\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    dataset.append(d)\n",
    "    \n",
    "# Split the data into training and validation sets\n",
    "data_train = dataset[:400000]\n",
    "data_valid =dataset[400000:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 uncooked (user,item) pair in validation\n",
    "valid = []\n",
    "\n",
    "for d in data_valid:\n",
    "    field = d['user_id,recipe_id,date,rating']\n",
    "    valid.append(field)\n",
    "    \n",
    "user_id = []\n",
    "item_id = []\n",
    "date = []\n",
    "rating = []\n",
    "for d in valid:\n",
    "    sl = d.split(\",\")\n",
    "    u = sl[0]\n",
    "    i = sl[1]\n",
    "    da = sl[2]\n",
    "    ra = sl[3]\n",
    "    user_id.append(u)\n",
    "    item_id.append(i)\n",
    "    date.append(da)\n",
    "    rating.append(ra)\n",
    "    \n",
    "unique_users = list(set(user_id))\n",
    "unique_items = list(set(item_id))\n",
    "    \n",
    "Pair_0 = []\n",
    "item_id_array = np.array(item_id)\n",
    "user_id_array = np.array(user_id)\n",
    "unique_items_array = np.array(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a negative entry by randomly choosing a recipe that user hasn’t cooked\n",
    "from random import sample\n",
    "for d in range(len(unique_users)):\n",
    "    #ct = ct+1\n",
    "    #print(ct)\n",
    "    item_1 = item_id_array[user_id_array == unique_users[d]]\n",
    "    #item_0 = [it for it in unique_items if it not in item_1.tolist()]  list difference  \n",
    "    item_0 = np.setdiff1d(unique_items_array, item_1).tolist()\n",
    "    pair_0 = [(unique_users[d],l) for l in sample(item_0,len(item_1))]\n",
    "    Pair_0 += pair_0\n",
    "   \n",
    "user_id_0 = []\n",
    "item_id_0 = []\n",
    "for (u,i) in Pair_0:\n",
    "    user_id_0.append(u)\n",
    "    item_id_0.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cook_0 = [0] * len(user_id_0)\n",
    "cook_1 = [1] * len(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the validation with 0 cooks to pandas dataframe\n",
    "valid_user_id_w0 = user_id + user_id_0\n",
    "valid_item_id_w0 = item_id + item_id_0\n",
    "valid_cook_w0 = cook_1 + cook_0\n",
    "\n",
    "dictionary = {'user_id': valid_user_id_w0,'recipe_id':valid_item_id_w0,'cook':valid_cook_w0}  \n",
    "valid_w0 = pd.DataFrame(dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Evaluate the performance (accuracy) of the baseline model on the trainning set\n",
    "train = []\n",
    "\n",
    "for d in data_train:\n",
    "    field = d['user_id,recipe_id,date,rating']\n",
    "    train.append(field)\n",
    "    \n",
    "user_id = []\n",
    "item_id = []\n",
    "date = []\n",
    "rating = []\n",
    "for d in train:\n",
    "    sl = d.split(\",\")\n",
    "    u = sl[0]\n",
    "    i = sl[1]\n",
    "    da = sl[2]\n",
    "    ra = sl[3]\n",
    "    user_id.append(u)\n",
    "    item_id.append(i)\n",
    "    date.append(da)\n",
    "    rating.append(ra)\n",
    "    \n",
    "cook_train = [1] * len(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train data to pandas dataframe\n",
    "dictionary = {'user_id': user_id,'recipe_id':item_id,'cook':cook_train}  \n",
    "train = pd.DataFrame(dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would-cook baseline: \n",
    "# just rank which recipes are popular and which are not, \n",
    "# and return '1' if a recipe is among the top-ranked\n",
    "\n",
    "Count = train.groupby('recipe_id')[['cook']].sum().reset_index()\n",
    "Count = Count.sort_values('cook', ascending=[False])\n",
    "totalCooked = train['cook'].sum()\n",
    "mostPopular = Count.values\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "  count += i\n",
    "  return1.add(ic)\n",
    "  if count > totalCooked/2: break\n",
    "\n",
    "pred_1 = valid_w0['recipe_id'].isin(return1)\n",
    "valid_w0['prediction'] = 0\n",
    "valid_w0.loc[pred_1,'prediction']= 1\n",
    "ac = valid_w0['cook'] == valid_w0['prediction']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Although we have built a validation set, it only consists of positive samples. For this task we also need examples of user/item pairs corresponding to recipes that weren’t cooked. For each entry (user,recipe) in the validation set, sample a negative entry by randomly choosing a recipe that user hasn’t cooked.1 Evaluate the performance (accuracy) of the baseline model on the validation set you have built (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61477"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.sum()/len(ac)\n",
    "# accuracy 0.62352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Assuming that the ‘non-made’ test examples are a random sample of user-recipe pairs, this threshold may not be the best one. See if you can find a better threshold and report its performance on your validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "  count += i\n",
    "  return1.add(ic)\n",
    "  if count > totalCooked/(3/5): break\n",
    "\n",
    "pred_1 = valid_w0['recipe_id'].isin(return1)\n",
    "valid_w0['prediction'] = 0\n",
    "valid_w0.loc[pred_1,'prediction']= 1\n",
    "ac = valid_w0['cook'] == valid_w0['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use 3/5 of the totalCooked, accuracy is: 0.551135\n"
     ]
    }
   ],
   "source": [
    "# accuracy 0.55215\n",
    "print('use 3/5 of the totalCooked, accuracy is:', ac.sum()/len(ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. An alternate baseline than the one provided might make use of the Jaccard similarity (or another sim- ilarity metric). Given a pair (u,g) in the validation set, consider all training items g′ that user u has cooked. For each, compute the Jaccard similarity between g and g′, i.e., users (in the training set) who have made g and users who have made g′. Predict as ‘made’ if the maximum of these Jaccard similarities exceeds a threshold (you may choose the threshold that works best). Report the performance on your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity\n",
    "# training set\n",
    "usersPerItem_train = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser_train = defaultdict(set) # Maps a user to the items that they rated\n",
    "cookDict_train = {} # To retrieve a rating for a specific user/item pair\n",
    "user_train = train['user_id'].values.tolist()\n",
    "item_train = train['recipe_id'].values.tolist()\n",
    "cook_train = train['cook'].values.tolist()\n",
    "\n",
    "for i in range(len(user_train)):\n",
    "    user_t = user_train[i]\n",
    "    item_t = item_train[i]\n",
    "    usersPerItem_train[item_t].add(user_t)\n",
    "    itemsPerUser_train[user_t].add(item_t)\n",
    "    cookDict_train[(user_t,item_t)] = cook_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "usersPerItem_valid = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser_valid = defaultdict(set) # Maps a user to the items that they rated\n",
    "cookDict_valid = {} # To retrieve a rating for a specific user/item pair\n",
    "user_valid = valid_w0['user_id'].values.tolist()\n",
    "item_valid = valid_w0['recipe_id'].values.tolist()\n",
    "cook_valid = valid_w0['cook'].values.tolist()\n",
    "\n",
    "for i in range(len(user_valid)):\n",
    "    user_v = user_valid[i]\n",
    "    item_v = item_valid[i]\n",
    "    usersPerItem_valid[item_v].add(user_v)\n",
    "    itemsPerUser_valid[user_v].add(item_v)\n",
    "    cookDict_valid[(user_v,item_v)] = cook_valid[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom\n",
    "\n",
    "sim_train = []\n",
    "for d,g in zip(user_train,item_train):\n",
    "    g_prime = itemsPerUser_train[d] - {g}\n",
    "    if len(g_prime) == 0:\n",
    "        sim_train.append(0)\n",
    "    else:\n",
    "        u = usersPerItem_train[g] - {d}\n",
    "        similarities = []\n",
    "        for g_ in list(g_prime):\n",
    "            u_ = usersPerItem_train[g_] - {d}      \n",
    "            similarities.append(Jaccard(u, u_))\n",
    "        sim = max(similarities)\n",
    "        sim_train.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let the median of similarities in the training be the shreshold\n",
    "Lambda = median(sim_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions using Jaccard in the validation\n",
    "\n",
    "sim_valid = [0] * len(user_valid)\n",
    "pred_valid = [0] * len(user_valid)\n",
    "ct = 0\n",
    "for d,g in zip(user_valid,item_valid):\n",
    "    if (d in set(user_train)) & (g in set(item_train)):\n",
    "        g_prime = itemsPerUser_train[d]\n",
    "        u = usersPerItem_train[g]\n",
    "        similarities = []\n",
    "        for g_ in list(g_prime):\n",
    "            u_ = usersPerItem_train[g_]        \n",
    "            similarities.append(Jaccard(u, u_))\n",
    "        sim = max(similarities)\n",
    "        sim_valid[ct] = sim\n",
    "        if sim > Lambda:\n",
    "            pred_valid[ct] = 1\n",
    "    ct = ct + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use Jaccard similarity, accuracy is: 0.570815\n"
     ]
    }
   ],
   "source": [
    "# the accuracy of Jaccrad Similarity Predictions on validation\n",
    "ac = sum((np.array(pred_valid) - np.array(cook_valid)) == 0) / len(pred_valid)\n",
    "print('use Jaccard similarity, accuracy is:', ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.Improve the above predictor by incorporating both a Jaccard-based threshold and a popularity based threshold. Report the performance on your validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopular = pd.DataFrame(mostPopular,columns = ['item_id','count'])\n",
    "mostPopular['cum_sum'] = mostPopular['count'].cumsum()\n",
    "\n",
    "Percentile_in_Mostpopular =[]\n",
    "\n",
    "for i in item_valid:\n",
    "    percentile = 1 - mostPopular.loc[mostPopular['item_id'] == i,'cum_sum']/400000\n",
    "    if len(percentile) ==0:\n",
    "        Percentile_in_Mostpopular.append(0)\n",
    "    else:\n",
    "        Percentile_in_Mostpopular.append(percentile.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(sim_valid).reshape(-1,1)\n",
    "b = np.array(Percentile_in_Mostpopular).reshape(-1,1)\n",
    "X = np.hstack((a,b))\n",
    "y = np.array(cook_valid).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiqi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/shiqi/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "pred_4 = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard_sim and Mostpopular as logistic Feature, accuracy is: 0.613595\n"
     ]
    }
   ],
   "source": [
    "ac = sum((np.array(pred_4) - np.array(cook_valid)) == 0) / len(cook_valid)\n",
    "print('Jaccard_sim and Mostpopular as logistic Feature, accuracy is:', ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.To run our model on the test set, we’ll have to use the files ‘stub Made.txt’ to find the user id/recipe id pairs about which we have to make predictions. Using that data, run the above model and upload your solution to Kaggle. Tell us your Kaggle user name (1 mark). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from test\n",
    "user_test = []\n",
    "item_test = []\n",
    "\n",
    "for l in open(\"stub_Made.txt\"):\n",
    "  if l.startswith(\"user_id\"):\n",
    "    #header\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  user_test.append(u)\n",
    "  item_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test similarity\n",
    "sim_test = [0] * len(user_test)\n",
    "\n",
    "ct = 0\n",
    "for d,g in zip(user_test,item_test):\n",
    "    if (d in set(user_train)) & (g in set(item_train)):\n",
    "        g_prime = itemsPerUser_train[d]\n",
    "        u = usersPerItem_train[g]\n",
    "        similarities = []\n",
    "        for g_ in list(g_prime):\n",
    "            u_ = usersPerItem_train[g_]        \n",
    "            similarities.append(Jaccard(u, u_))\n",
    "        sim = max(similarities)\n",
    "        sim_test[ct] = sim\n",
    "    ct = ct + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute percentile of test\n",
    "Percentile_in_Mostpopular_test =[]\n",
    "\n",
    "for i in item_test:\n",
    "    percentile = 1 - mostPopular.loc[mostPopular['item_id'] == i,'cum_sum']/400000\n",
    "    if len(percentile) ==0:\n",
    "        Percentile_in_Mostpopular_test.append(0)\n",
    "    else:\n",
    "        Percentile_in_Mostpopular_test.append(percentile.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(sim_test).reshape(-1,1)\n",
    "b = np.array(Percentile_in_Mostpopular_test).reshape(-1,1)\n",
    "X_test = np.hstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_5 = clf.predict(X_test)\n",
    "\n",
    "dictionary = {'user_id': user_test,'recipe_id':item_test,'prediction':pred_5}  \n",
    "dataframe = pd.DataFrame(dictionary)\n",
    "dataframe['user_id-recipe_id'] = dataframe[['user_id', 'recipe_id']].apply(lambda x: '-'.join(x), axis=1)\n",
    "df = dataframe[['user_id-recipe_id','prediction']]\n",
    "df.to_csv('test_pred.csv',index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Kaggle user id: ShiqiB, accuracy in test is 0.7088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9 Fit a predictor of the form rating(user,item) ~ alpha + B_u + B_i, by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization\n",
    "parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 \n",
    "# use the trainning data\n",
    "\n",
    "train = []\n",
    "\n",
    "for d in data_train:\n",
    "    field = d['user_id,recipe_id,date,rating']\n",
    "    train.append(field)\n",
    "    \n",
    "user_id = []\n",
    "item_id = []\n",
    "date = []\n",
    "rating = []\n",
    "for d in train:\n",
    "    sl = d.split(\",\")\n",
    "    u = sl[0]\n",
    "    i = sl[1]\n",
    "    da = sl[2]\n",
    "    ra = sl[3]\n",
    "    user_id.append(u)\n",
    "    item_id.append(i)\n",
    "    date.append(da)\n",
    "    rating.append(ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple (bias only) latent factor-based recommender\n",
    "import scipy\n",
    "import scipy.optimize    \n",
    "\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "\n",
    "for u, i in zip(user_id,item_id):\n",
    "    itemsPerUser[u].append(i)\n",
    "    usersPerItem[i].append(u)\n",
    "    \n",
    "labels = [int(d) for d in rating]\n",
    "ratingMean = sum(labels) / len(labels)\n",
    "\n",
    "N = len(train)\n",
    "nUsers = len(itemsPerUser)\n",
    "nItems = len(usersPerItem)\n",
    "users = list(itemsPerUser.keys())\n",
    "items = list(usersPerItem.keys())\n",
    "    \n",
    "alpha = ratingMean\n",
    "\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "    \n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(u, i) for u,i in zip(user_id,item_id)]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    " \n",
    "    \n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for u,i,r in zip(user_id,item_id,labels):\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - r\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for d in userBiases:\n",
    "        dUserBiases[d] += 2*lamb*userBiases[d]\n",
    "    for p in itemBiases:\n",
    "        dItemBiases[p] += 2*lamb*itemBiases[p]\n",
    "    dtheta = [dalpha] + [dUserBiases[d] for d in users] + [dItemBiases[p] for p in items]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.8987313599958769\n",
      "MSE = 0.8856358581692653\n",
      "MSE = 0.8986218156135961\n",
      "MSE = 0.8986223585785804\n",
      "MSE = 0.8986218283827365\n",
      "MSE = 0.8986218159274242\n",
      "MSE = 0.8986218156213774\n",
      "MSE = 0.8986218156138821\n",
      "MSE = 0.8986218156135792\n",
      "MSE = 0.8986218156135792\n",
      "MSE = 0.8986218156137126\n",
      "MSE = 0.8986218156135766\n",
      "MSE = 0.8986218156135765\n",
      "MSE = 0.8986218156137227\n",
      "MSE = 0.898621815613584\n",
      "MSE = 0.8986218156135765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.58080000e+00, -6.90340584e-05, -6.47033902e-06, ...,\n",
       "        -1.16446855e-06,  8.40470414e-07, -1.16446855e-06]),\n",
       " 0.8986658023286048,\n",
       " {'grad': array([ 1.58518187e-04, -4.20211595e-07, -3.23406545e-08, ...,\n",
       "         -5.80250330e-09,  7.32538905e-09, -6.02742858e-09]),\n",
       "  'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 16,\n",
       "  'nit': 2,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE on the validation set\n",
    "valid = []\n",
    "\n",
    "for d in data_valid:\n",
    "    field = d['user_id,recipe_id,date,rating']\n",
    "    valid.append(field)\n",
    "    \n",
    "user_id_valid = []\n",
    "item_id_valid = []\n",
    "date_valid = []\n",
    "rating_valid = []\n",
    "for d in valid:\n",
    "    sl = d.split(\",\")\n",
    "    u = sl[0]\n",
    "    i = sl[1]\n",
    "    da = sl[2]\n",
    "    ra = sl[3]\n",
    "    user_id_valid.append(u)\n",
    "    item_id_valid.append(i)\n",
    "    date_valid.append(da)\n",
    "    rating_valid.append(ra)\n",
    "    \n",
    "valid_pred = []\n",
    "for u,i in zip(user_id_valid,item_id_valid):\n",
    "    if (u in user_id) & (i in item_id):\n",
    "        pred = prediction(u,i)\n",
    "    else:\n",
    "        pred = 0\n",
    "    valid_pred.append(pred)\n",
    "\n",
    "\n",
    "valid_labels = [int(d) for d in rating_valid]\n",
    "mse = MSE(valid_pred, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the validation when lambda = 1: 4.424182115391335\n"
     ]
    }
   ],
   "source": [
    "print('MSE on the validation when lambda = 1:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10.Report the user and recipe IDs that have the largest and smallest values of β (1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id with smallest value: 70705426\n",
      "recipe_id with smallest value: 29147042\n"
     ]
    }
   ],
   "source": [
    "min_i = min(itemBiases, key=itemBiases.get)\n",
    "min_u = min(userBiases, key=userBiases.get)\n",
    "print('user_id with smallest value:', min_u)\n",
    "print('recipe_id with smallest value:', min_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id with largest value: 32445558\n",
      "recipe_id with largest value: 98124873\n"
     ]
    }
   ],
   "source": [
    "max_i = max(itemBiases, key=itemBiases.get)\n",
    "max_u = max(userBiases, key=userBiases.get)\n",
    "print('user_id with largest value:', max_u)\n",
    "print('recipe_id with largest value:', max_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11.Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.8987313599956165\n",
      "MSE = 0.8856358581703624\n",
      "MSE = 1.0746606639793392\n",
      "MSE = 0.8832581770796194\n",
      "MSE = 0.8771058156117816\n",
      "MSE = 0.873528718514732\n",
      "MSE = 0.862320034372388\n",
      "MSE = 0.8512624763515296\n",
      "MSE = 0.8444868753600093\n",
      "MSE = 0.8228016646938193\n",
      "MSE = 0.8067877503919217\n",
      "MSE = 0.7856040265163795\n",
      "MSE = 0.7726711764546196\n",
      "MSE = 0.7645020637406588\n",
      "MSE = 0.7526559733162275\n",
      "MSE = 2.332933414419877\n",
      "MSE = 0.7525687420785727\n",
      "MSE = 0.7485172844293243\n",
      "MSE = 0.745751976602477\n",
      "MSE = 0.7388906345384213\n",
      "MSE = 0.7337763733758575\n",
      "MSE = 0.7286398545569337\n",
      "MSE = 0.7184982274027016\n",
      "MSE = 0.7065799398162318\n",
      "MSE = 0.6997977568019986\n",
      "MSE = 0.6902915107464591\n",
      "MSE = 0.6867562582280317\n",
      "MSE = 0.7256021731133394\n",
      "MSE = 0.6866777931626011\n",
      "MSE = 0.6851839999427446\n",
      "MSE = 0.6799994596446733\n",
      "MSE = 0.6779274036896351\n",
      "MSE = 0.676086853630142\n",
      "MSE = 0.674883892415983\n",
      "MSE = 0.6770513151037971\n",
      "MSE = 0.6720589736972918\n",
      "MSE = 0.6687911089972433\n",
      "MSE = 0.6666579688920391\n",
      "MSE = 0.6666345947454777\n",
      "MSE = 0.6663721228203081\n",
      "MSE = 0.6662611708427993\n",
      "MSE = 0.6646315658276623\n",
      "MSE = 0.665365485185672\n",
      "MSE = 0.6649435628032088\n",
      "MSE = 0.6636489422563526\n",
      "MSE = 0.662869015019863\n",
      "MSE = 0.6623093516340538\n",
      "MSE = 0.6617112901792307\n",
      "MSE = 0.6608004173703657\n",
      "MSE = 0.7136398360419018\n",
      "MSE = 0.6605886061828161\n",
      "MSE = 0.6596442923911604\n",
      "MSE = 0.6590301449894912\n",
      "MSE = 0.6585681800950719\n",
      "MSE = 0.6581646802552261\n",
      "MSE = 0.8061926911118547\n",
      "MSE = 0.657639488900999\n",
      "MSE = 0.6562515724803488\n",
      "MSE = 0.653739337713004\n",
      "MSE = 0.6512661576843809\n",
      "MSE = 0.6498476336719361\n",
      "MSE = 0.6497122072652348\n",
      "MSE = 0.6496992596443261\n",
      "MSE = 0.649555774582887\n",
      "MSE = 0.6491707890001457\n",
      "MSE = 0.6491287738454582\n",
      "MSE = 0.6471803019085769\n",
      "MSE = 0.6497124514573975\n",
      "MSE = 0.6473228268300472\n",
      "MSE = 0.6472097079557814\n",
      "MSE = 0.647283433613744\n",
      "MSE = 0.6472243964912752\n",
      "MSE = 0.6472571226358583\n",
      "MSE = 0.6470661316860383\n",
      "MSE = 0.6470985419908312\n",
      "MSE = 0.6468612472597617\n",
      "MSE = 0.6486538133998795\n",
      "MSE = 0.647034306918414\n",
      "MSE = 0.6468891761953071\n",
      "MSE = 0.6468660693153662\n",
      "MSE = 0.646862089298762\n",
      "MSE = 0.6468613945882993\n",
      "MSE = 0.6468612730460518\n",
      "MSE = 0.6468612517731809\n",
      "MSE = 0.6468612480496547\n",
      "MSE = 0.6468612473978466\n",
      "MSE = 0.6468612472838883\n",
      "MSE = 0.6468612472639983\n",
      "MSE = 0.6468612472604318\n",
      "MSE = 0.6468612472598851\n",
      "MSE = 0.6468612472597864\n",
      "MSE = 0.646861247259762\n",
      "MSE = 0.6468612472597625\n",
      "MSE = 0.646861247259762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.44422718,  0.0110169 , -0.17181633, ..., -0.05755092,\n",
       "         0.02968592,  0.00505276]),\n",
       " 0.6920247959690112,\n",
       " {'grad': array([-4.39403112e-04,  1.82387726e-05, -2.42875820e-06, ...,\n",
       "          6.63247725e-07, -6.40542839e-07,  1.79803282e-07]),\n",
       "  'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 94,\n",
       "  'nit': 64,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels,0.00001)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = []\n",
    "for u,i in zip(user_id_valid,item_id_valid):\n",
    "    if (u in user_id) & (i in item_id):\n",
    "        pred = prediction(u,i)\n",
    "    else:\n",
    "        pred = 0\n",
    "    valid_pred.append(pred)\n",
    "\n",
    "\n",
    "valid_labels = [int(d) for d in rating_valid]\n",
    "mse = MSE(valid_pred, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the validation when lambda = 0.00001: 4.3677161976797985\n"
     ]
    }
   ],
   "source": [
    "print('MSE on the validation when lambda = 0.00001:', mse)\n",
    "# lambda = 0.00001: mse = 4.3638\n",
    "# lambda = 0.001: mse = 4.3995\n",
    "# lambda = 100: mse = 4.4242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions in test data\n",
    "user_test = []\n",
    "item_test = []\n",
    "\n",
    "for l in open(\"stub_Rated.txt\"):\n",
    "  if l.startswith(\"user_id\"):\n",
    "    #header\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  user_test.append(u)\n",
    "  item_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for u,i in zip(user_test,item_test):\n",
    "    if (u in user_id) & (i in item_id): # in the training data\n",
    "        pred = prediction(u,i)\n",
    "    else:\n",
    "        pred = 0\n",
    "    test_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'user_id': user_test,'recipe_id':item_test,'prediction':test_pred}  \n",
    "dataframe = pd.DataFrame(dictionary)\n",
    "dataframe['user_id-recipe_id'] = dataframe[['user_id', 'recipe_id']].apply(lambda x: '-'.join(x), axis=1)\n",
    "df = dataframe[['user_id-recipe_id','prediction']]\n",
    "df.to_csv('test_pred.csv',index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
